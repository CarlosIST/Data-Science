{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Frequentist_AB_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Frequentist A/B Testing\n",
        "\n",
        "**Notebook by Carlos SantillÃ¡n**\n",
        "\n",
        "**Based on Vatsal's article in TowardsDataScience**\n",
        "\n",
        "*The purpose of this notebook is to serve me as a reference guide when this material is needed*\n",
        "\n",
        "**April 2022**\n"
      ],
      "metadata": {
        "id": "VgmHWjdF05QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A/B testing is commonly used across all industries to make decision in different aspects of the business. From writing emails, to choosing landing pages, implementing specific feature designs, A/B testing can be used to make the best decision based on statistical analysis."
      ],
      "metadata": {
        "id": "NEIt12Mx1uOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is A/B Testing?\n"
      ],
      "metadata": {
        "id": "kPIg46Bd10Oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferential statistics is often used to infer something about the population based on the observations on a sample of that population. A/B testing is the application of inferential statistics for researching user experience. It consists of randomized experiments with two variants, A and B [1]. \n",
        "\n",
        "Generally, by testing each variant against user response, one can find statistical evidence proving that one variant is a better choice than the other or you can conclude that there is no statistical significance from choosing 1 over the other. \n",
        "\n",
        "Common applications a company would have to conduct A/B tests would be make decisions to improve conversion rates of their users, improve marketability of their products, increases their daily active users, etc."
      ],
      "metadata": {
        "id": "yUfghvgT14_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequentist Approach"
      ],
      "metadata": {
        "id": "YI5CSKgo2BpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the more traditional approach when it comes to statistical inference, it is commonly introduced in basic stats courses taken in university. A simple outline of this approach can be of the following manner:\n",
        "\n",
        "1. Identify the null and alternative hypothesis\n",
        "  - **Null hypothesis** : there is no significant difference between specified populations, any observed difference being due to sampling or experimental error.\n",
        "  - **Alternative hypothesis** : a hypothesis which contradicts the null hypothesis\n",
        "\n",
        "2. Calculate a sample size to achieve statistical significance (usually 95%)\n",
        "\n",
        "\n",
        "3. Calculate the test statistics and map it to a p value\n",
        "\n",
        "\n",
        "4. Accept or reject the null hypothesis based on if the p value is smaller / larger than the p critical value"
      ],
      "metadata": {
        "id": "WuIS6VKE2MRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Null & Alternative Hypothesis"
      ],
      "metadata": {
        "id": "LBdsG0WO2dOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying a hypothesis to test is generally through domain knowledge of your given problem. \n",
        "\n",
        "The **null hypothesis is generally a statement regarding the population that is believed to be true**. \n",
        "\n",
        "The **alternative hypothesis is a statement which contradicts the null hypothesis**. \n",
        "\n",
        "A simple example can be outlined in the following scenario; you want to increase the conversion rate of users visiting your website based on adding a distinct feature. The null hypothesis would be that adding this distinct feature to the website will have no impact on the conversion rate. The alternative hypothesis would be that adding this new feature will impact conversion rate."
      ],
      "metadata": {
        "id": "XhhaUlXi2rt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Mean Estimate"
      ],
      "metadata": {
        "id": "DaCdbSGY23hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample mean estimate from a group of observations is essentially an estimate of the population mean. It can be represented by the following formula :\n",
        "\n",
        "$$\\mu = \\frac{1}{N} \\sum^{N}_{i=1} x_{i}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $N$ represents the total number of samples\n",
        "\n",
        "- $x_{i}$ represents the number of occurences of an event"
      ],
      "metadata": {
        "id": "tb3Dx8Pp26qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an ideal situation we would want the difference between the sample mean estimates of the variations (A and B) to be high. The larger the difference between the two would indicate a larger gap between the test statistics, implying that there would be a clear winner between the variations."
      ],
      "metadata": {
        "id": "R9UGqJPh3KBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Intervals"
      ],
      "metadata": {
        "id": "WjC8fLnn3b8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence intervals are a range of values so defined that there is a specified probability that the value of a parameter lies within it. It can be outlined by the following formula :\n",
        "\n",
        "$$\\left [ \\hat{\\mu}-t_{\\alpha, N-1} \\frac{\\hat{\\sigma}}{\\sqrt{N}},\\; \\hat{\\mu}+t_{\\alpha, N-1} \\frac{\\hat{\\sigma}}{\\sqrt{N}} \\right ]$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\mu$ represents the sample mean estimate\n",
        "\n",
        "- $t$ is the confidence level value\n",
        "\n",
        "- $\\sigma$ is the sample standard deviation\n",
        "\n",
        "- $N$ is the sample size"
      ],
      "metadata": {
        "id": "xZXqak1N3fgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Statistics"
      ],
      "metadata": {
        "id": "e1O24k4K4P6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test statistic is a point value on a normal distribution, which shows how far off (in no. of standard deviations) from the mean the test-statistic is. There are various formulations of the test statistic based on the sample size and other factors. A few variations of the formula can be seen in the image below."
      ],
      "metadata": {
        "id": "vadZOpN_4Tyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Test for** | $H_{0}$  | **Test Statistic**  | **Use when**  |\n",
        "|---|---|---|---|\n",
        "| Pop. mean $\\mu$  | $\\mu = mu_{0}$  | $z =\\frac{\\bar{x}-\\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}$  | Normal dist. or $n > 30$, $\\sigma$ known  |\n",
        "| Pop. mean $\\mu$  | $\\mu = mu_{0}$  | $t =\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}$  | $n < 30$ and/or $\\sigma$ unknown  |\n",
        "| Pop. prop. $p$  | $p = p_{0}$  | $z=\\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}$  | $n\\hat{p} \\geq 10,\\; n(1-\\hat{p}) \\geq 10$|"
      ],
      "metadata": {
        "id": "8eqguUqBSAXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the value yielded from the test statistic, one can map the test-statistic to a p-value and either accept of reject the hypothesis based on if the p value is above or below the p critical value."
      ],
      "metadata": {
        "id": "jeXgWO2qUs2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### P-Value"
      ],
      "metadata": {
        "id": "DewepTtSU2WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics, a *p-value* is the probability that the null hypothesis (the idea that a theory being tested is false) gives for a specific experimental result to happen. p-value is also called probability value. \n",
        "\n",
        "- If the p-value is low, the null hypothesis is unlikely, and the experiment has statistical significance as evidence for a different theory. \n",
        "\n",
        "In many fields, an experiment must have a p-value of less than 0.05 for the experiment to be considered evidence of the alternative hypothesis. In short, **a low p-value means a higher chance of the null hypothesis being false.** As explained above, once a p-value is identified, interpreting the results is fairly simple."
      ],
      "metadata": {
        "id": "bhYEtf2EU36e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coffee Cup Explanation"
      ],
      "metadata": {
        "id": "ljD_R5QlVTv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A new study says that if you drink more than four cups a day, it might -- it might -- increase the risk of an early death. >> At this coffee shop in Atlanta today, excitement is brewing over new research showing that morning cup of Joe may help you live longer. Just one cup a day lowered the chance by 12%. Bump that to three cups a day and the risk drops by 18%. >> So if you're drinking coffee right now, you're probably wondering which study to believe.\n",
        "\n",
        "Well, today I'm going to show you how to judge the true strength of the evidence so that you can make sense of conflicting data. The key is to recognize when the evidence is exaggerated. Well, to start with, a scientific study is always more persuasive when the results are replicated or confirmed in a second independent study. A bullseye on the first shot might be lucky, but two in a row is probably not a random accident. Unfortunately, the two coffee studies totally contradict one another. \n",
        "\n",
        "In fact, there are so many examples of contradictory studies that one prominent scientist suggested that most published research could be false. For example, the FDA took a look at 22 cases where the results of phase 3 contradicted the results of phase 3. Some scientists even think we might have a replication crisis. \n",
        "\n",
        "Well, the main character in this controversy is the gatekeeper of P-values called statistical significance. A p-value is a value between 0 and 1 that tells you how surprising the data is if there's no effect other than random variation. Now, smaller p-values represent stronger evidence of a real effect. Let's look at an example from one of the early cholesterol drugs. Pravastatin reduces the risk of coronary events by 31% in patients with high cholesterol who've never had a heart attack. The p-value for Pravastatin is less than .001. This means that we would expect to see a 31% reduction in coronary events purely by chance, less than one in a thousand times. \n",
        "\n",
        "So this data's too surprising to be random luck. And it's evidence of a real drug effect. Now, the key point is that smaller p-values represent stronger evidence of a real drug effect. Now, this begs the question, when are p-values small enough to represent statistically significant evidence? Well, the traditional threshold for statistical significance is .05 or one in 20. Now, p-values less than .05 represent drug effects that are large enough to be surprising and -- and worth a little bit more research, because we would expect that purely by chance less than 20 times. It's enough proof for publication. \n",
        "\n",
        "Now, in the p-values above .05, that represents drug effects that are more likely to be random noise, and usually they wind up unpublished in the researcher's trash can. Now, some scientists believe the .05 threshold allows too many false discoveries to slip into the published literature. This sparked a debate on whether to ban p-values in statistical significance altogether. But p-values are not to blame. The replication crisis is a result of unrealistic expectations that grow out of exaggerated evidence. Here's the simplest guide I can give you for setting realistic expectations based on the evidence. \n",
        "\n",
        "P-values less than .05 or one in 20 are evidence of something surprising and they invite a second study. P-values less than one in a hundred represent strong evidence. And p-values less than one in a thousand represent proof beyond a reasonable doubt, and they probably don't need independent confirmation. This is why some drugs are approved just based on a single study. Now, let's look a little deeper at what to expect when the strength of evidence is right around .05 or one in 20 where replication is not as certain. Suppose we test a thousand drugs in independent clinical trials. \n",
        "\n",
        "The 10 blocks of a hundred little pills. Assume 10% are effective, the hundred in the white block, and assume 90% are ineffective, the 900 in the red blocks. Now, our gatekeeper correctly identifies most of the effective drugs for publication, but he sends a few effective drugs into the trash can by mistake. He filters out almost all of the ineffective drugs into the trash can, as he should, and only 5% of the ineffective drugs slip by him and are mistakenly published as meaningful discoveries. Now, bear in mind many of these false discoveries get cleaned up over time as we conduct more studies. \n",
        "\n",
        "So statistical significance is not a guarantee of replication, but it helps us to identify most of the good drugs without becoming overwhelmed by too many false discoveries. Unfortunately, there are two silent killers of replication that exaggerate the evidence, and they trick the gatekeeper into allowing false discoveries to slip through. And these are multiplicity and selection bias. \n",
        "\n",
        "Let's start with multiplicity. We ask almost unlimited questions of the data from a single clinical trial. The number of end points, doses, subgroups and other factors quickly multiply. It's like bunny rabbits, into 20, 30, 40, 50, even hundreds of p-values. For example, in this list of 40 p-values, we naturally focus on the smallest ones like the one in red. It looks really impressive, but the true strength of evidence depends on the number of questions that we asked. When you take into account the 40 questions, the strength of evidence is no longer proof beyond doubt. \n",
        "\n",
        "It's simply something surprising and worth another look. Now, without the context of the number of questions, p-values can exaggerate the evidence and really look stronger than they really are. Now, selection bias is the other silent killer of replication, and it really hurts the gatekeeper. When we ask multiple questions, we have a tendency to switch our focus after we see the data to the best outcomes. It's like shooting first and then painting a bullseye around the hole like that was our target all along. \n",
        "\n",
        "Now, let's look at an example from a drug for high blood pressure. PRAISE-1 and PRAISE-2 were clinical trials evaluating amlodopine in heart patients. It had a promising effect in mortality in one subgroup of patients. You might expect replication based on the p-value in red which is less than one in a thousand. But the PRAISE-2 model did not replicate the mortality findings observed in the PRAISE-1 trial. Why not? What happened? Investigators switched end points and then focused on the best of eight subgroups in a clinical trial with no overall effect. So that p-value in red exaggerated the evidence and probably didn't support conducting a second trial."
      ],
      "metadata": {
        "id": "KS1W2-MUWAV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Example"
      ],
      "metadata": {
        "id": "CdpF9b-oYD6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mortgage department of a large bank is interested in the nature of loans of first-time borrowers. \n",
        "\n",
        "This information will be used to tailor their marketing strategy. They believe that 50% of first-time borrowers take out smaller loans than other borrowers. \n",
        "\n",
        "They perform a hypothesis test to determine if the percentage is the same or different from 50%. They sample 100 first-time borrowers and find 53 of these loans are smaller that the other borrowers. For the hypothesis test, they choose a 5% level of significance."
      ],
      "metadata": {
        "id": "ZfHvVVAIYZIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis**: $p = 0.05$\n",
        "\n",
        "**Alternate Hypotehsis**: $p \\neq 0.05$\n",
        "\n",
        "This will be run as a two tailed test:"
      ],
      "metadata": {
        "id": "QrDO1TlNYgSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "## calculate the confidence interval and p critical value\n",
        "significance_level = 0.05\n",
        "conf_interval = 1 - (significance_level / 2)\n",
        "\n",
        "p_crticial = norm.ppf(conf_interval)\n",
        "print(p_crticial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TtUAFepjZptD",
        "outputId": "f4996ef8-eeb5-4680-83bd-740b1446f7c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.959963984540054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that our significance level is 5% and that it is a two tailed test, our confidence interval will be 1â0.05/2 = 0.975. From running the code above you will yield a p-critical value of 1.96"
      ],
      "metadata": {
        "id": "h-EllRIqZstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## test statistics\n",
        "n = 100   \n",
        "p1 = 53 / 100\n",
        "p0 = 50 / 100\n",
        "p = 0.5\n",
        "q = 1 - p\n",
        "\n",
        "def test_statistic(n, p1, p0, p, q):\n",
        "    '''\n",
        "    Calculates the test statistic of a normal disribution \n",
        "    '''\n",
        "    z = (p1 - p0) / np.sqrt((p * q) / n)\n",
        "    return z\n",
        "\n",
        "test_statistic(n , p1, p0, p, q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jT_A2JCBZzW1",
        "outputId": "ab09d29f-8031-4e48-f2af-fef17fba1c89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6000000000000005"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the code above we notice that the test statistic is 0.6. This is barely off of the mean of the standard normal distribution of zero. There is virtually no difference from the sample proportion and the hypothesized proportion in terms of standard deviations."
      ],
      "metadata": {
        "id": "KYOR1qtCZ2Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test statistic is within the critical values, hence we fail to reject the null hypothesis. This implies that at a 95% level of significance we cannot reject the null hypothesis that 50% of first-time borrowers have the same size loans as other borrowers"
      ],
      "metadata": {
        "id": "XVy2UKGYZ8Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concluding Remarks"
      ],
      "metadata": {
        "id": "eHsaeD6iaEM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summation, the frequentist approach to A/B testing is used to make a decision based on statistical significance of the outcome favouring one of the two variants A or B. This is done through identifying the null and alternative hypothesis associated to the test, identifying the sample size and calculating the test statistic at a certain confidence interval. Once the test statistic is obtained we can determine the P value and conclude if we accept or reject the null hypothesis."
      ],
      "metadata": {
        "id": "GpJohRRzaNAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6h0IMgNSaQX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}